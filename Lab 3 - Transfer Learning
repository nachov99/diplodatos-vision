{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Lab 3 - Transfer Learning","private_outputs":true,"provenance":[{"file_id":"1dYPDjGGntic3QZEACf-cvZudAG7Bw57r","timestamp":1605855998715},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb","timestamp":1572893915470}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pXaUCkQcQKXF"},"source":["# Transfer Learning\n","\n","Este tutorial fue elaborado a partir de las siguientes fuentes:\n","\n","* [Deep Learning For Beginners Using Transfer Learning In Keras](https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e)\n","* [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n","* [Transfer Learning Introduction](https://www.hackerearth.com/practice/machine-learning/transfer-learning/transfer-learning-intro/tutorial/)\n","* [Keras: Feature extraction on large datasets with Deep Learning](https://www.pyimagesearch.com/2019/05/27/keras-feature-extraction-on-large-datasets-with-deep-learning/)\n","* [Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)\n"]},{"cell_type":"markdown","metadata":{"id":"NT3jVsg-hmub"},"source":["## ¿Qué es Tranfer Learning?\n","\n","Si bien el entrenamiento de una CNN desde cero es posible para proyectos con datasets pequeños, la mayoría de las aplicaciones requieren enormes cantidades de datos procesados ​​y mucha potencia computacional. Ahí es donde entra en juego *transfer learning* (TL) o *aprendizaje de transferencia*. Su definición es la siguiente:\n","\n","> **Situación en la que se aprovecha lo aprendido en un entorno para mejorar la generalización en otro entorno**\n","\n","En el contexto de deep learnig, TL implica tomar un modelo ya entrenado para un problema específico, y usar sus parámetros como parte del aprendizaje de un nuevo modelo en otro problema diferente.\n","\n","De esta manera, TL tiene las siguiente ventajas:\n","\n","1. Se puede entrenar con **datasets mucho más pequeños**.\n","2. Requiere **mucha menos potencia computacional**, ya que usamos parámetros pre-entrenados."]},{"cell_type":"markdown","metadata":{"id":"d-THgjbMlzNF"},"source":["## ¿Por qué TL funciona bien?\n","\n","Como vimos en los laboratorios anteriores, el objetivo de entrenar una CNN es encontrar los valores óptimos en cada una de las matrices de filtro que se han definido en la arquitectura de la red (es decir, los parámetros del modelo). Cuando observamos qué han aprendido a reconocer los filtros en cada capa de la CNN, o por qué se activa cada filtro, podemos ver cosas realmente interesantes.\n","\n","![](https://miro.medium.com/max/2177/1*jPCEik198_CjtmSL2H6o4g.png)\n","\n","Los filtros en las primeras capas de la CNN aprenden a reconocer **colores** y ciertas **líneas horizontales y verticales**. Las siguientes capas aprenden a reconocer **formas sencillas** usando las líneas y los colores aprendidos en las capas anteriores. \n","\n","![](https://miro.medium.com/max/1963/1*1Y6HZxK-lOmqB8KnizTCow.png)\n","\n","Luego, las siguientes capas aprenden a reconocer **texturas**, luego **partes de objetos** como piernas, ojos, nariz, etc. Finalmente, los filtros en las últimas capas se activan por **objetos completos** como perros, automóviles, etc.\n","\n","Cuando los investigadores comenzaron a observar que es lo que las CNN aprendían, notaron que redes entrenadas con datasets de imágenes diferentes exhibían un fenómeno curioso en común: las características de las primeras capas **no parecen ser específicas de un dataset en particular, si que son genéricas**, y por lo tanto son aplicables a muchos problemas. \n","\n","Y es éste fenómeno el que se aprovecha para hacer TL cuando usamos CNN para procesamiento de imágenes. Básicamente, primero se entrena una *red base* sobre un *dataset base*, y luego se reutilizan las características aprendidas en una segunda *red específica* para que se entrene con un nuevo *dataset específico*. Este proceso tenderá a funcionar si las características son generales, es decir, adecuadas para el problema original y problema el nuevo."]},{"cell_type":"markdown","metadata":{"id":"hRTa3Ee15WsJ"},"source":["# Transfer Learning con una CNN pre-entrenada\n","\n","\n","Tutorial extraído de https://www.tensorflow.org/tutorials/images/transfer_learning"]},{"cell_type":"markdown","metadata":{"id":"n4RN67Isbzb9"},"source":["En este tutorial vamos a clasificar imágenes de gatos vs perros utilizando TL de una red pre-entrenada. Como ya vimos antes, la intuición detrás de TL es que si un modelo es entrenado con un dataset lo suficientemente grande y general, el modelo servirá efectivamente como un modelo genérico del *mundo visual*. Luego, se puede aprovechar los *feature maps* aprendidos sin tener que comenzar desde cero a entrenar un nuevo modelo.\n","\n","En este laboratorio, vamos a ver dos formas de hacer TL con un modelo previamente entrenado:\n","\n","1. **Extracción de características**: utiliza los filtros aprendidos por una red para extraer características significativas de nuevas imágenes. Simplemente se agrega un nuevo clasificador que se entrenará desde cero, encima del modelo previamente entrenado para que pueda reutilizar los feature maps aprendidos previamente para un nuevo dataset.\n","![](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-07-at-9-15-21-pm.png?w=1496)\n","No es necesario (re) entrenar todo el modelo. La CNN base ya contiene características que son genéricamente útiles para clasificar imágenes. Sin embargo, la parte final de la clasificación del modelo preentrenado es específica de la tarea de clasificación original y, posteriormente, específica del conjunto de clases en las que se entrenó el modelo.\n","\n","2. **Ajuste fino**: *descongela* algunas de las capas superiores de una CNN base y entrena conjuntamente las capas del clasificador recién agregadas y las últimas capas de la CNN base. Esto permite *ajustar* las representaciones de características de orden superior en el modelo base para hacerlas más relevantes para la tarea específica.\n","\n","El flujo de trabajo general que vamos a seguir es el siguiente:\n","\n","1. Examinar y comprender los datos.\n","2. Crear una *tubería de entrada*, en este caso utilizando el objeto `ImageDataGenerator` de la librería Keras.\n","3. Compliar el nuevo modelo: carga modelo base previamente entrenado y agregar nuevas capas de clasificación en la parte superior.\n","4. Entrenar el nuevo modelo.\n","5. Evaluar el modelo."]},{"cell_type":"code","metadata":{"id":"iBMcobPHdD8O"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import os\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqOt6Sv7AsMi"},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","keras = tf.keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v77rlkCKW0IJ"},"source":["## Preprocesamiento de los datos"]},{"cell_type":"markdown","metadata":{"id":"0GoKGm1duzgk"},"source":["### Descargar los datos"]},{"cell_type":"markdown","metadata":{"id":"vHP9qMJxt2oz"},"source":["Vamos a utilizar [TensorFlow Datasets](http://tensorflow.org/datasets) para cargar el dataset de gatos y perros.\n","\n","El paquete `tfds` es la forma más fácil de cargar datos predefinidos. Si tiene sus propios datos y está interesado en importarlos con TensorFlow, se pude consultar el tutorial [Load images](https://www.tensorflow.org/tutorials/load_data/images)\n"]},{"cell_type":"code","metadata":{"id":"KVh7rDVAuW8Y"},"source":["import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nsoic6bGuwQ-"},"source":["El método `tfds.load` descarga y almacena los datos en caché, y devuelve un objeto` tf.data.Dataset`. Estos objetos proporcionan métodos potentes y eficientes para manipular datos y canalizarlos a un modelo.\n","\n","Primero tenemos que dividir el dataset `cats_vs_dog` en 3 conjuntos: entrenamiento, validación y prueba. Para esto usamos la función de subdivisión para dividir el dataset: 80%, 10%, 10% de los datos en cada conjunto respectivamente."]},{"cell_type":"code","metadata":{"id":"ro4oYaEmxe4r"},"source":["# Split the dataset into train, validation and test\n","splits=['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n","\n","(raw_train, raw_validation, raw_test), metadata = tfds.load(\n","    'cats_vs_dogs', split=list(splits), with_info=True, as_supervised=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o29EfE-p0g5X"},"source":["Los objetos resultantes `tf.data.Dataset` contienen pares `(imagen, etiqueta) `. Donde las imágenes tienen *forma variable* y 3 canales, y la etiqueta es un escalar."]},{"cell_type":"code","metadata":{"id":"GIys1_zY1S9b"},"source":["num_classes = metadata.features['label'].num_classes\n","print(\"Number of classes:\", metadata.features['label'].num_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yO1Q2JaW5sIy"},"source":["Mostramos las 3 primeras imágenes y etiquetas del conjunto de entrenamiento:"]},{"cell_type":"code","metadata":{"id":"K5BeQyKThC_Y"},"source":["get_label_name = metadata.features['label'].int2str\n","\n","for image, label in raw_train.take(3):\n","  plt.figure()\n","  plt.imshow(image)\n","  plt.title(get_label_name(label))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvidPx6jeFzf"},"source":["### Formatear los datos\n","\n","Usamos el módulo `tf.image` para formatear las imágenes para nuestro problema. Primero cambiamos el tamaño de las imágenes a un tamaño de entrada fijo, y luego cambiamos la escala de los canales de entrada a un rango entre `[-1,1]`."]},{"cell_type":"code","metadata":{"id":"y3PM6GVHcC31"},"source":["IMG_SIZE = 160 # All images will be resized to 160x160\n","\n","def format_example(image, label):\n","  image = tf.cast(image, tf.float32)\n","  image = (image/127.5) - 1\n","  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n","  return image, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2MRh_AeBtOM"},"source":["Aplicamos el formateo a cada imagen en el dataset utilizando el método de `map` de la clase `tf.data.Dataset`:"]},{"cell_type":"code","metadata":{"id":"SFZ6ZW7KSXP9"},"source":["train = raw_train.map(format_example)\n","validation = raw_validation.map(format_example)\n","test = raw_test.map(format_example)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E5ifgXDuBfOC"},"source":["Ahora mezclamos y procesamos los datos por lotes."]},{"cell_type":"code","metadata":{"id":"Yic-I66m6Isv"},"source":["BATCH_SIZE = 32\n","SHUFFLE_BUFFER_SIZE = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3UUPdm86LNC"},"source":["train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","validation_batches = validation.batch(BATCH_SIZE)\n","test_batches = test.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02rJpcFtChP0"},"source":["Inspeccionamos un lote de datos:"]},{"cell_type":"code","metadata":{"id":"iknFo3ELBVho"},"source":["for image_batch, label_batch in train_batches.take(1):\n","   pass\n","\n","image_batch.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OkH-kazQecHB"},"source":["## Crear la modelo base a partir de una CNN previamente entrenada\n","\n","Ahora vamos a crear nuestro modelo base a partir de la CNN **MobileNet V2**, desarrollada en Google. \n","\n","![](https://machinethink.net/images/mobilenet-v2/Classifier@2x.png)\n","\n","Esta CNN está pre-entrenada con el dataset [ImageNet](https://image-net.org), un dataset con 1.4 millones de imágenes distribuídas en 1000 clases. ImageNet tiene un conjunto de datos para investigación bastante variado, con categorías como `jackfruit` y` syringe`. Sin embargo, esta *base de conocimiento* nos ayudará a distinguir a los gatos y los perros en nuestro dataset específico.\n","\n","Primero, debemos elegir qué capa de *MobileNet V2* se usará para la extracción de características. Obviamente, la última capa de clasificación no es muy útil. En su lugar, vamos a a tomar la última capa antes de la operación de aplanar (`flatten`). Esta capa se llama *bottleneck layer* (cuello de botella). Las características de la capa bottleneck conservan mucha generalidad en comparación con la capa final.\n","\n","Para esto vamos a crear una instancia del modelo MobileNet V2 con parámetros ya entrenados sobre el dataset ImageNet. Al especificar el argumento `include_top = False`, carga la red sin incluir las capas de clasificación, lo que es ideal para la extracción de características."]},{"cell_type":"code","metadata":{"id":"19IQ2gqneqmS"},"source":["IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n","\n","# Create the base model from the pre-trained model MobileNet V2\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                               include_top=False,\n","                                               weights='imagenet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AqcsxoJIEVXZ"},"source":["Este extractor de características convierte cada imagen de `160x160x3` en un bloque de features de `5x5x1280`. Podemos ver esto reflejado en la forma del lote de imágenes de ejemplo:"]},{"cell_type":"code","metadata":{"id":"Y-2LJL0EEUcx"},"source":["feature_batch = base_model(image_batch)\n","print(feature_batch.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlx56nQtfe8Y"},"source":["## Extracción de características\n","\n","Ahora vamos a *congelar* la *base convolucional* creada a partir de la etapa anterior, y luego vamos a usar eso como un extractor de características, añadir un clasificador en la parte superior de la misma y entrenar el nuevo clasificador."]},{"cell_type":"markdown","metadata":{"id":"CnMLieHBCwil"},"source":["### Congelar la base convolucional\n","\n","Es importante congelar la base convolucional antes de compilar y entrenar el modelo. Al congelar evitamos que los parámetros en una capa determinada se actualicen durante el entrenamiento. MobileNet V2 tiene muchas capas, por lo que vamos a establecer el argumento `layer.trainable = False` para congelar todas las capas."]},{"cell_type":"code","metadata":{"id":"OTCJH4bphOeo"},"source":["base_model.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpbzSmPkDa-N"},"source":["# Let's take a look at the base model architecture\n","base_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wdMRM8YModbk"},"source":["### Agregar un nuevo clasificador"]},{"cell_type":"markdown","metadata":{"id":"QBc31c4tMOdH"},"source":["Para generar predicciones a partir del bloque de características vamos a diseñar una nueva ANN.\n","\n","La primera capa de la ANN toma el promedio para cada feature map de `5x5` que entrega la última capa de MabileNet V2. Para esto agregamos al modelo una capa `tf.keras.layers.GlobalAveragePooling2D`, que convierte los features maps en un solo vector de 1280 elementos por imagen."]},{"cell_type":"code","metadata":{"id":"dLnpMF5KOALm"},"source":["global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","feature_batch_average = global_average_layer(feature_batch)\n","print(feature_batch_average.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1p0OJBR6dOT"},"source":["Agregamos una capa `tf.keras.layers.Dense` para convertir estas características en una sola predicción por imagen. No necesita una función de activación aquí porque esta predicción se tratará como un `logit`: los números positivos predicen la clase 1, los números negativos predicen la clase 0."]},{"cell_type":"code","metadata":{"id":"Wv4afXKj6cVa"},"source":["prediction_layer = keras.layers.Dense(1)\n","prediction_batch = prediction_layer(feature_batch_average)\n","print(prediction_batch.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iqnBeZrfoIc"},"source":["Ahora apilamos el extractor de características y estas dos capas usando un modelo `tf.keras.Sequential`:"]},{"cell_type":"code","metadata":{"id":"eApvroIyn1K0"},"source":["model = tf.keras.Sequential([\n","  base_model,\n","  global_average_layer,\n","  prediction_layer\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0ylJXE_kRLi"},"source":["### Compilar el modelo\n","\n","Debemos compilar el modelo antes de entrenarlo. Como hay dos clases, usamos una función de pérdida binaria."]},{"cell_type":"code","metadata":{"id":"RpR8HdyMhukJ"},"source":["base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8ARiyMFsgbH"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxOcmVr0ydFZ"},"source":["Los 2.5M de parámetros en MobileNet están congelados, pero hay 1.2K *parámetros entrenables* en la capa densa."]},{"cell_type":"markdown","metadata":{"id":"RxvgOYTDSWTx"},"source":["### Entrenar el modelo\n","\n","Después de entrenar durante 10 epoch, deberíamos obtener una precisión cercana al 96%."]},{"cell_type":"code","metadata":{"id":"hlHEavK7DUI7"},"source":["SPLIT_WEIGHTS = (0.8, 0.1, 0.1)\n","\n","num_train, num_val, num_test = (\n","  metadata.splits['train'].num_examples*weight\n","  for weight in SPLIT_WEIGHTS\n",")\n","\n","print(num_train)\n","print(num_val)\n","print(num_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Om4O3EESkab1"},"source":["initial_epochs = 10\n","steps_per_epoch = round(num_train)//BATCH_SIZE\n","validation_steps = 20\n","\n","loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsaRFlZ9B6WK"},"source":["history = model.fit(train_batches,\n","                    epochs=initial_epochs,\n","                    validation_data=validation_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hd94CKImf8vi"},"source":["### Curvas de aprendizaje\n","\n","Veamos las curvas de aprendizaje para accuracy/loss sobre los conjuntos de entrenamiento y validación cuando se usa el modelo base MobileNet V2 como un extractor de características fijas."]},{"cell_type":"code","metadata":{"id":"53OTCh3jnbwV"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TZTwG7nhm0C"},"source":["## Resumen:\n","\n","**Uso de un modelo previamente entrenado para la extracción de características**: Cuando se trabaja con un dataset pequeño, es común aprovechar las características aprendidas por un modelo entrenado en un dateset más grande en el mismo dominio del problema. Esto se hace creando instancias del modelo pre-entrenado y agregando un clasificador completamente conectado en la parte superior (al final). El modelo pre-entrenado está *congelado* y solo los pesos del clasificador se actualizan durante el entrenamiento.\n","En este caso, la base convolucional extrajo todas las características asociadas con cada imagen y solo entrenó a un clasificador que determina la clase de imagen dado ese conjunto de características extraídas."]},{"cell_type":"markdown","metadata":{"id":"OtMf_wkJtIBi"},"source":["---\n","\n","## **Ejercicio 3.1** (OPCIONAL)\n","\n","Utilizar MobileNet v2 y los conceptos de Transfer Learning revisados en el tutorial anterior para entrenar un clasificador binario para el dataset que puede [descargar en este link](https://drive.google.com/file/d/11CTzx2PVxMhU4mvBWOZcJAx1nRM03l86/view?usp=sharing).\n","\n","**NOTA**: Recuerden que deben reemplazar la cadena \"/content/drive/My Drive/PATH_TO_DATASET\" por el path al directorio del dataset"]},{"cell_type":"code","metadata":{"id":"j2cjbpJGtePe"},"source":["# EJERCICIO 1\n","\n","# Montamos la unida de Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Seteamos el directorio de trabajo \n","%cd \"/content/drive/My Drive/PATH_TO_DATASET\"\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-7EtPeDPLyp"},"source":["# Importamos librerias de Python\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","keras = tf.keras\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","from PIL import Image\n","import IPython.display as display\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pathlib\n","import tensorflow_datasets as tfds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAsBwdmgQk6V"},"source":["## FUNCIONES AUXILIARES\n","\n","# Obtiene la etiqueta de una imagen a partir del directorio que la contiene\n","def get_label(file_path):\n","  # convert the path to a list of path components\n","  parts = tf.strings.split(file_path, '/')\n","  # The second to last is the class-directory\n","  return parts[-2] == CLASS_NAMES\n","\n","# Procesa una imagen para adecuarla a la estructura de datos de entrada de la CNN\n","def decode_img(img):\n","  # convert the compressed string to a 3D uint8 tensor\n","  img = tf.image.decode_jpeg(img, channels=3)\n","  # Use `convert_image_dtype` to convert to floats in the [0,1] range (luego se multiplica por 2 y se resta 1 para dejarlo en el rango [-1,1]).\n","  img = (tf.image.convert_image_dtype(img, tf.float32)*2)-1\n","  # resize the image to the desired size.\n","  return tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n","\n","# Devuelve una imagen y su etiqueta a partir del directorio que recibe como argumento\n","def process_path(file_path):\n","  label = get_label(file_path)\n","  # load the raw data from the file as a string\n","  img = tf.io.read_file(file_path)\n","  img = decode_img(img)\n","  return img, label  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5FmZ4q7OnLl"},"source":["# Path al directorio que contiene el dataset de imágenes\n","data_dir = pathlib.Path(\"/content/drive/My Drive/PATH_TO_DATASET\")\n","\n","image_count = len(list(data_dir.glob('*/*.jpg')))\n","print(\"Cantidad de imágenes:\", image_count)\n","CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n","print(\"Clases:\", CLASS_NAMES)\n","\n","# Carga la lista de imágenes \n","list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n","\n","# Tamaño al que se van a redimensionar las imágenes\n","IMG_SIZE = 160\n","\n","# Carga las imagenes del dataset junto a sus etiquetas y lo desordena\n","labeled_ds = list_ds.map(process_path)\n","labeled_ds.shuffle(1000)\n","\n","# Divide el dataset en 3 conjuntos\n","train_size = int(0.6 * image_count)\n","val_size = int(0.2 * image_count)\n","test_size = int(0.2 * image_count)\n","\n","print(\"Cantidad de imágenes para entrenamiento:\", train_size)\n","print(\"Cantidad de imágenes para validación:\", val_size)\n","print(\"Cantidad de imágenes para testeo:\", test_size)\n","\n","train = labeled_ds.take(train_size)\n","test = labeled_ds.skip(train_size)\n","val = test.skip(val_size)\n","test = test.take(test_size)\n","\n","# Muestra 4 imágenes tomadas aleatoriamente del dataset \n","for image, label in labeled_ds.take(4):\n","  plt.figure()\n","  plt.imshow(image)\n","  plt.title(CLASS_NAMES[label.numpy()])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QB5rc2jdhqDy"},"source":["# Crea los BATCHES para cada conjunto\n","BATCH_SIZE = 10\n","SHUFFLE_BUFFER_SIZE = 1000\n","\n","train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n","validation_batches = val.batch(BATCH_SIZE)\n","test_batches = test.batch(BATCH_SIZE)\n","\n","# Create the base model from the pre-trained model MobileNet V2\n","IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n","base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n","                                               include_top=False,\n","                                               weights='imagenet')\n","\n","# Congela el modelo base (feature extraction)\n","base_model.trainable = False\n","\n","# Let's take a look at the base model architecture\n","base_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Boz3tmsFhqE3"},"source":["# Crea la primera capa de la ANN que vamos a usar como clasificador\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","feature_batch_average = global_average_layer(feature_batch)\n","\n","# Crea la capa de salida de la ANN que vamos a usar como clasificador\n","prediction_layer = keras.layers.Dense(1)\n","prediction_batch = prediction_layer(feature_batch_average)\n","\n","# Apila el extractor de características y las dos capas de la ANN\n","model = tf.keras.Sequential([\n","  base_model,\n","  global_average_layer,\n","  prediction_layer\n","])\n","\n","# Compila el modelo antes de entrenarlo. Como hay dos clases usamos una función de pérdida binaria\n","base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Let's take a look at the complete model architecture\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JIVtrohkghm"},"source":["initial_epochs = 10\n","steps_per_epoch = round(train_size)//BATCH_SIZE\n","validation_steps = 20\n","\n","history = model.fit(train_batches,\n","                    epochs=initial_epochs,\n","                    validation_data=validation_batches)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgxI1vClkgii"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPE3uBlithq0"},"source":["## **Ejercicio 3.2** (OPCIONAL)\n","\n","Utilizar MobileNet v2 como modelo base como extractor de características y reemplace el clasificador construido mediante una ANN en el tutorial anterior con un clasificador [SVM](https://scikit-learn.org/stable/modules/svm.html). Verificar la performance del clasificador para el dataset `dogs_vs_cats` y dataset del ejemplo anterior."]},{"cell_type":"code","metadata":{"id":"6eQxZoH_uKzJ"},"source":["# EJERCICIO 2\n"],"execution_count":null,"outputs":[]}]}